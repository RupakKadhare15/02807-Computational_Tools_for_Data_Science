{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16bc6d3",
   "metadata": {},
   "source": [
    "# Step 1 - FARS Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058e6fd",
   "metadata": {},
   "source": [
    "Remember to download the raw data files from https://www.nhtsa.gov/file-downloads?p=nhtsa/downloads/FARS/2023/National/\n",
    "- On the website, download *FARS2023NationalCSV.zip*\n",
    "- Add the following files to *Dataset* folder: vehicle, person, accident\n",
    "\n",
    "-------------------------\n",
    "\n",
    "This is the code for the first step of our pipeline.\n",
    "In summary:\n",
    "* Join the 3 files data (Person, Vehicle, Accident), with the greatest granularity possible (based in person)\n",
    "* Delete duplicate features\n",
    "* Replace the \"Not reported\" or \"unknown\" codes of the features with null values\n",
    "\n",
    "Missing code handling was a particular challenge as there is no universal value for labelling the \"unknown\" or \"not reported\" values. Some features detect the \"unknown\" as 9999, others as 9, and others as \"\". This caused the value replacement to be more complex, as we could not take a general approach. For example, we could not replace all 9 or 99 with nulls because it could be the case that a driver was 99 years old or a 9 year old was injured, and, by replacing this value, information was going to be lost. Therefore, the FARS User Manual was reviewed and the following code was developed in order to handle this situation from a better perspective.\n",
    "\n",
    "A limitation of this approach is that, as some specific missing values were mapped, the code only works on the most recent years of the dataset. In last editions of the database (Ex. before 2008), many features had a different code for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8aaf08",
   "metadata": {},
   "source": [
    "### How to run the code:\n",
    "\n",
    "1) Run libraries\n",
    "2) Run all the sections in order (top to bottom)\n",
    "3) Run the Use\n",
    "4) Optional: review the code of each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cc2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e4ef5",
   "metadata": {},
   "source": [
    "### Configuration for the field-specific missing codes\n",
    "\n",
    "Specific missing codes and known numeric columns in FARS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d9d46",
   "metadata": {},
   "source": [
    "##### FARS field-specific missing codes (based on FARS User Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02298f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FARS field-specific missing codes (based on FARS User Manual)\n",
    "\n",
    "FIELD_SPECIFIC_MISSING = {\n",
    "    \n",
    "    # Demographics\n",
    "    \"AGE\": [998, 999],  # Here the example of the introduction can be seen: 9, 88, 98, 99 are possible ages, so they are not included\n",
    "    \"SEX\": [8, 9],\n",
    "    \"RACE\": [97, 98, 99],  # Discontinued in 2018\n",
    "    \"HISPANIC\": [8, 9, 98, 99],  # 0 as no Fatality, in case other type of analysis wants to be made\n",
    "    \n",
    "    # Temporal\n",
    "    \"HOUR\": [99],\n",
    "    \"MINUTE\": [99],\n",
    "    \"DAY\": [99],     # Changed in 2010, but kept in case some raw data was incorrectly labeled\n",
    "    \"MONTH\": [99],   # Changed in 2009, but kept in case some raw data was incorrectly labeled\n",
    "    \"DAY_WEEK\": [9], # Changed in 2010, but kept in case some raw data was incorrectly labeled\n",
    "    \n",
    "    # Injury/Outcome\n",
    "    \"INJ_SEV\": [9],\n",
    "    \"DEATH_HR\": [88, 99],\n",
    "    \"DEATH_MN\": [88, 99],\n",
    "    \n",
    "    # Behaviors\n",
    "    \"DRINKING\": [8, 9, 98, 99],\n",
    "    \"DRUGS\": [8,9],\n",
    "    \"DRUG_DET\": [8],\n",
    "    \"SPEEDREL\": [8, 9],\n",
    "    \"REST_USE\": [20, 96, 98, 99],\n",
    "    \"REST_MIS\": [7, 8],\n",
    "    \"AIR_BAG\": [98, 99],\n",
    "    \"EJECTION\": [7, 8, 9],\n",
    "    \"EJ_PATH\": [9],\n",
    "    \"EXTRICAT\": [9],\n",
    "    \n",
    "    # Vehicle\n",
    "    \"BODY_TYP\": [98, 99],\n",
    "    \"MOD_YEAR\": [9998, 9999],\n",
    "    \"TOW_VEH\": [9],\n",
    "    \"J_KNIFE\": [9],\n",
    "    \"MCARR_I1\": [0, 77, 99],\n",
    "    \"MCARR_I2\": [000000000, 777777777, 999999999],\n",
    "    \"GVWR\": [0, 8, 9],   # Discontinued in 2019\n",
    "    \"V_CONFIG\": [0, 98, 99],\n",
    "    \"CARGO_BT\": [0, 28, 98, 99],\n",
    "    \"HAZ_PLAC\": [0, 8],\n",
    "    \"HAZ_ID\": [0, 8888],\n",
    "    \"HAZ_CNO\": [0, 88],\n",
    "    \"HAZ_REL\": [0, 8],\n",
    "    \"BUS_USE\": [98, 99],\n",
    "    \"SPEC_USE\": [99],\n",
    "    \"EMER_USE\": [0, 8, 9],\n",
    "    \"ROLLOVER\": [8],\n",
    "    \"ROLINLOC\": [8, 9],\n",
    "    \"IMPACT1\": [98, 99],\n",
    "    \"FIRE_EXP\": [9],\n",
    "    \n",
    "    # Environmental\n",
    "    \"WEATHER\": [98, 99],\n",
    "    \"WEATHER1\": [98, 99],  # Discontinued in 2019\n",
    "    \"WEATHER2\": [98, 99],  # Discontinued in 2019\n",
    "    \"LGT_COND\": [8,9],\n",
    "    \n",
    "    # Location\n",
    "    \"ROUTE\": [99],\n",
    "    \"RUR_URB\": [8, 9],\n",
    "    \"FUNC_SYS\": [98, 99],\n",
    "    \"RD_OWNER\": [98, 99],\n",
    "    \"MILEPT\": [99998, 99999],\n",
    "    \"LATITUDE\": [77.7777000, 88.8888000, 99.9999000],\n",
    "    \"LONGITUD\": [777.7777000, 888.8888000, 999.9999000],\n",
    "    \n",
    "    # Roadway\n",
    "    \"SP_JUR\": [9],\n",
    "    \"HARM_EV\": [98, 99],\n",
    "    \"MAN_COLL\": [98, 99],\n",
    "    \"RELJCT1\": [8, 9],\n",
    "    \"RELJCT2\": [98, 99],\n",
    "    \"TYP_INT\": [98, 99],\n",
    "    \"WRK_ZONE\": [9],\n",
    "    \"REL_ROAD\": [98, 99],\n",
    "    \"NOT_HOUR\": [88, 99],\n",
    "    \"NOT_MIN\": [88, 98, 99],\n",
    "    \"ARR_HOUR\": [88, 99],\n",
    "    \"ARR_MIN\": [88, 98, 99],\n",
    "    \"HOSP_HR\": [88, 99],\n",
    "    \"HOSP_MN\": [88, 98, 99],\n",
    "    \n",
    "    # Person-specific\n",
    "    \"PER_TYP\": [19],\n",
    "    \"SEAT_POS\": [98, 99],\n",
    "    \"SEATING\": [98, 99],\n",
    "    \"HELM_USE\": [20, 98, 99],\n",
    "    \"HELM_MIS\": [7, 8],\n",
    "    \"DRUGRES1\": [95, 999],  # Discontinued in 2017\n",
    "    \"DRUGRES2\": [95, 999],  # Discontinued in 2017\n",
    "    \"DRUGRES3\": [95, 999],  # Discontinued in 2017\n",
    "    \"DSTATUS\": [8, 9],\n",
    "    \n",
    "    # Crash characteristics\n",
    "    \"CF1\": [98, 99],\n",
    "    \"CF2\": [98, 99],\n",
    "    \"CF3\": [98, 99],\n",
    "    \"VTRAFWAY\": [8, 9],\n",
    "    \"VNUM_LAN\": [8, 9],\n",
    "    \"VSPD_LIM\": [98, 99],\n",
    "    \"VALIGN\": [9],\n",
    "    \"VPROFILE\": [8, 9],\n",
    "    \"VPAVETYP\": [8, 9],\n",
    "    \"VSURCOND\": [98, 99],\n",
    "    \"VTRAFCON\": [97, 99],\n",
    "    \"VTCONT_F\": [8, 9],\n",
    "    \n",
    "    # Default codes (be aware that there are many features that contain 8,9 as missing values)\n",
    "    # In the case of this project, due to its scope and purpose, the above features were confirmed to have a correct code replacement\n",
    "    # In case of a full variables consideration, all should be checked manually\n",
    "    \"_DEFAULT\": [88, 98, 99, 997, 998, 999, 9999]\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b8cf1",
   "metadata": {},
   "source": [
    "##### Numeric columns (common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b82033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default numeric columns if not specified\n",
    "NUMERIC_COLUMNS = [\n",
    "    # IDs\n",
    "    \"ST_CASE\", \"VEH_NO\", \"PER_NO\", \"STATE\", \"COUNTY\",\n",
    "    \n",
    "    # Demographics\n",
    "    \"AGE\", \"SEX\", \"RACE\", \"HISPANIC\",\n",
    "    \n",
    "    # Temporal\n",
    "    \"YEAR\", \"MONTH\", \"DAY\", \"DAY_WEEK\", \"HOUR\", \"MINUTE\",\n",
    "    \n",
    "    # Outcomes\n",
    "    \"INJ_SEV\", \"DEATH_HR\", \"DEATH_MN\", \"FATALS\", \"PERSONS\",\n",
    "    \n",
    "    # Person behavior\n",
    "    \"PER_TYP\", \"SEAT_POS\", \"REST_USE\", \"REST_MIS\", \"AIR_BAG\",\n",
    "    \"EJECTION\", \"EJ_PATH\", \"EXTRICAT\", \"DRINKING\", \"ALC_RES\",\n",
    "    \"ALC_STATUS\", \"ATST_TYP\", \"ALC_DET\", \"DRUGS\", \"DRUG_DET\",\n",
    "    \"DSTATUS\", \"HOSPITAL\", \"DOA\",\n",
    "    \n",
    "    # Vehicle\n",
    "    \"BODY_TYP\", \"MOD_YEAR\", \"TOW_VEH\", \"SPEC_USE\", \"EMER_USE\",\n",
    "    \"ROLLOVER\", \"ROLINLOC\", \"IMPACT1\", \"FIRE_EXP\", \"SPEEDREL\",\n",
    "    \"DR_DRINK\", \"DR_SF\",\n",
    "    \n",
    "    # Environmental\n",
    "    \"WEATHER\", \"WEATHER1\", \"WEATHER2\", \"LGT_COND\", \"SCH_BUS\",\n",
    "    \n",
    "    # Location\n",
    "    \"ROUTE\", \"RUR_URB\", \"FUNC_SYS\", \"RD_OWNER\",\n",
    "    \"MILEPT\", \"LATITUDE\", \"LONGITUD\", \"SP_JUR\",\n",
    "    \n",
    "    # Crash\n",
    "    \"HARM_EV\", \"MAN_COLL\", \"RELJCT1\", \"RELJCT2\", \"TYP_INT\",\n",
    "    \"WRK_ZONE\", \"REL_ROAD\", \"NOT_HOUR\", \"NOT_MIN\", \"ARR_HOUR\",\n",
    "    \"ARR_MIN\", \"HOSP_HR\", \"HOSP_MN\", \"CF1\", \"CF2\", \"CF3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf136d",
   "metadata": {},
   "source": [
    "### Step 1.1: Data Loading\n",
    "The original FARS CSV files are loaded with the columns as strings to prevent conversion issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88a8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fars_csv(filename: str, data_dir: Path) -> pl.DataFrame:\n",
    "    path = Path(data_dir) / filename\n",
    "    \n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    \n",
    "    # Load with no schema inference\n",
    "    df = pl.read_csv(path, infer_schema_length=0)\n",
    "    \n",
    "    print(f\"Loaded: {filename}\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ad68d",
   "metadata": {},
   "source": [
    "### Step 1.2: Key Validation\n",
    "\n",
    "Remove duplicates and review null keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd1fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_keys(\n",
    "    df: pl.DataFrame,\n",
    "    key_columns: List[str]\n",
    ") -> pl.DataFrame:\n",
    "    # Check for NULL keys\n",
    "    for col in key_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        null_count = df.filter(pl.col(col).is_null()).height\n",
    "        if null_count > 0:\n",
    "            print(f\"NULL values in {col}: {null_count:,}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    original_count = df.height\n",
    "    df_unique = df.unique(subset=key_columns, maintain_order=True)\n",
    "    duplicates = original_count - df_unique.height\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(f\"Removed {duplicates:,} duplicate rows\")\n",
    "    else: \n",
    "        print(f\"No duplicates found\")\n",
    "    \n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d13e2",
   "metadata": {},
   "source": [
    "### Step 1.3: Data Type Conversion\n",
    "\n",
    "Convert the specified columns from string to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e23d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(\n",
    "    df: pl.DataFrame,\n",
    "    columns: List[str]\n",
    ") -> pl.DataFrame:\n",
    "\n",
    "    converted = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Skip if already numeric\n",
    "        if df[col].dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "                             pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "                             pl.Float32, pl.Float64]:\n",
    "            converted += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = df.with_columns(\n",
    "                pl.col(col).cast(pl.Int32, strict=False).alias(col)\n",
    "            )\n",
    "            converted += 1\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"Converted {converted} columns to numeric\")\n",
    "    if errors > 0:\n",
    "        print(f\"Errors in {errors} columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f16db",
   "metadata": {},
   "source": [
    "### Step 1.4: Missing Code Handling\n",
    "Use the field-specific missing codes dictionary to apply the missing code handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f77699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_field_specific_missing_codes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    total_replaced = 0\n",
    "    columns_modified = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Get field-specific missing codes\n",
    "        if col in FIELD_SPECIFIC_MISSING:\n",
    "            missing_codes = FIELD_SPECIFIC_MISSING[col]\n",
    "        else:\n",
    "            # Skip columns not in numeric list and not explicitly defined\n",
    "            if col not in NUMERIC_COLUMNS:\n",
    "                continue\n",
    "            missing_codes = FIELD_SPECIFIC_MISSING[\"_DEFAULT\"]\n",
    "        \n",
    "        try:\n",
    "            col_dtype = df[col].dtype\n",
    "            \n",
    "            if col_dtype == pl.Utf8:\n",
    "                # String column\n",
    "                missing_code_strs = [str(c) for c in missing_codes if isinstance(c, int)]\n",
    "                if \"\" in missing_codes:\n",
    "                    missing_code_strs.append(\"\")\n",
    "                \n",
    "                count_before = df.filter(pl.col(col).is_in(missing_code_strs)).height\n",
    "                \n",
    "                if count_before > 0:\n",
    "                    df = df.with_columns(\n",
    "                        pl.when(pl.col(col).is_in(missing_code_strs))\n",
    "                        .then(None)\n",
    "                        .otherwise(pl.col(col))\n",
    "                        .alias(col)\n",
    "                    )\n",
    "                    total_replaced += count_before\n",
    "                    columns_modified += 1\n",
    "            \n",
    "            elif col_dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "                               pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "                               pl.Float32, pl.Float64]:\n",
    "                # Numeric column\n",
    "                numeric_codes = [c for c in missing_codes if isinstance(c, (int, float))]\n",
    "                \n",
    "                count_before = df.filter(pl.col(col).is_in(numeric_codes)).height\n",
    "                \n",
    "                if count_before > 0:\n",
    "                    df = df.with_columns(\n",
    "                        pl.when(pl.col(col).is_in(numeric_codes))\n",
    "                        .then(None)\n",
    "                        .otherwise(pl.col(col))\n",
    "                        .alias(col)\n",
    "                    )\n",
    "                    total_replaced += count_before\n",
    "                    columns_modified += 1\n",
    "        \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\" Replaced missing codes in {columns_modified} columns\")\n",
    "    print(f\" Total replacements: {total_replaced:,}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c11d97",
   "metadata": {},
   "source": [
    "### Step 1.5: Dataset Integration\n",
    "\n",
    "* Integrate person, vehicle, and accident datasets into person-level data\n",
    "\n",
    "* Join hierarchy: person (base) → vehicle → accident\n",
    "\n",
    "* Overlapped columns are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987d18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_person_vehicle_accident(\n",
    "    person: pl.DataFrame,\n",
    "    vehicle: pl.DataFrame,\n",
    "    accident: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "\n",
    "    # Identify overlapping columns\n",
    "    vehicle_cols = set(vehicle.columns)\n",
    "    person_cols = set(person.columns)\n",
    "    accident_cols = set(accident.columns)\n",
    "    \n",
    "    # Drop overlapping columns from vehicle (keep join keys)\n",
    "    vehicle_person_overlap = (vehicle_cols & person_cols) - {\"ST_CASE\", \"VEH_NO\"}\n",
    "    if vehicle_person_overlap:\n",
    "        vehicle = vehicle.drop(list(vehicle_person_overlap))\n",
    "    \n",
    "    # Drop overlapping columns from accident (keep join keys)\n",
    "    accident_person_overlap = (accident_cols & person_cols) - {\"ST_CASE\"}\n",
    "    if accident_person_overlap:\n",
    "        accident = accident.drop(list(accident_person_overlap))\n",
    "    \n",
    "    # Join person → vehicle\n",
    "    per_veh = person.join(vehicle, on=[\"ST_CASE\", \"VEH_NO\"], how=\"left\")\n",
    "    \n",
    "    # Join person-vehicle → accident\n",
    "    per_full = per_veh.join(accident, on=\"ST_CASE\", how=\"left\")\n",
    "    \n",
    "    return per_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee6c1e",
   "metadata": {},
   "source": [
    "### Step 1.6: Data Quality Summary (optional)\n",
    "\n",
    "Null value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e4236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_summary(df: pl.DataFrame, title: str = \"Data Summary\"):\n",
    "\n",
    "    # NULL analysis\n",
    "    null_counts = {}\n",
    "    for col in df.columns:\n",
    "        null_count = df.filter(pl.col(col).is_null()).height\n",
    "        if null_count > 0:\n",
    "            null_pct = (null_count / df.shape[0]) * 100\n",
    "            null_counts[col] = (null_count, null_pct)\n",
    "    \n",
    "    print(f\"\\nColumns with NULLs: {len(null_counts)}/{len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe0009",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Run the complete preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4546450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step1_pipeline(\n",
    "    data_dir: Path,\n",
    "    output_file: Optional[Path] = None,\n",
    "    numeric_columns: Optional[List[str]] = None\n",
    ") -> pl.DataFrame:\n",
    "    \n",
    "    # Check for numeric columns\n",
    "    if numeric_columns is None:\n",
    "        numeric_columns = NUMERIC_COLUMNS\n",
    "\n",
    "    # 1.1: Load data\n",
    "    accident = load_fars_csv(\"accident.csv\", data_dir)\n",
    "    vehicle = load_fars_csv(\"vehicle.csv\", data_dir)\n",
    "    person = load_fars_csv(\"person.csv\", data_dir)\n",
    "    \n",
    "    # 1.2: Validate keys\n",
    "    accident = remove_duplicate_keys(accident, [\"ST_CASE\"])\n",
    "    vehicle = remove_duplicate_keys(vehicle, [\"ST_CASE\", \"VEH_NO\"])\n",
    "    person = remove_duplicate_keys(person, [\"ST_CASE\", \"VEH_NO\", \"PER_NO\"])\n",
    "    \n",
    "    # 1.3: Convert to numeric\n",
    "    accident = convert_to_numeric(accident, numeric_columns)\n",
    "    vehicle = convert_to_numeric(vehicle, numeric_columns)\n",
    "    person = convert_to_numeric(person, numeric_columns)\n",
    "    \n",
    "    # 1.4: Handle missing codes\n",
    "    accident = replace_field_specific_missing_codes(accident)\n",
    "    vehicle = replace_field_specific_missing_codes(vehicle)\n",
    "    person = replace_field_specific_missing_codes(person)\n",
    "    \n",
    "    # 1.5: Integrate datasets\n",
    "    df_integrated = join_person_vehicle_accident(person, vehicle, accident)\n",
    "    \n",
    "    # 1.6: Quality summary (optional)\n",
    "    print_data_summary(df_integrated, \"Integrated Person-Level Dataset\")\n",
    "    \n",
    "    # 1.7: Save output\n",
    "    if output_file is not None:\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save parquet\n",
    "        df_integrated.write_parquet(output_path)\n",
    "        print(f\"Saved parquet to: {output_path}\")\n",
    "\n",
    "        # Save csv (optional, just for check) !!! EXTREMELY HEAVY\n",
    "        # csv_path = output_path.with_suffix('.csv')\n",
    "        # df_integrated.write_csv(csv_path)\n",
    "        # print(f\"Saved csv to: {csv_path}\")\n",
    "    \n",
    "    print(f\"Final dataset: {df_integrated.shape[0]:,} rows × {df_integrated.shape[1]} columns\")\n",
    "    \n",
    "    return df_integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b7cf5",
   "metadata": {},
   "source": [
    "### Use\n",
    "\n",
    "- The paths are set for use with a cloned repository\n",
    "- Modify them as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7198c16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: accident.csv\n",
      "Shape: 37,654 rows × 80 columns\n",
      "Loaded: vehicle.csv\n",
      "Shape: 58,319 rows × 203 columns\n",
      "Loaded: person.csv\n",
      "Shape: 92,400 rows × 126 columns\n",
      "No duplicates found\n",
      "No duplicates found\n",
      "No duplicates found\n",
      "Converted 35 columns to numeric\n",
      "Converted 20 columns to numeric\n",
      "Converted 44 columns to numeric\n",
      " Replaced missing codes in 23 columns\n",
      " Total replacements: 160,816\n",
      " Replaced missing codes in 31 columns\n",
      " Total replacements: 524,756\n",
      " Replaced missing codes in 34 columns\n",
      " Total replacements: 630,350\n",
      "\n",
      "Columns with NULLs: 237/337\n",
      "Saved parquet to: Dataset\\St1_person_level_integrated.parquet\n",
      "Final dataset: 92,400 rows × 337 columns\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"Dataset\")\n",
    "OUTPUT_FILE = Path(\"Dataset/St1_person_level_integrated.parquet\")\n",
    "\n",
    "# Run complete pipeline\n",
    "df = run_step1_pipeline(\n",
    "    data_dir=DATA_DIR,\n",
    "    output_file=OUTPUT_FILE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
