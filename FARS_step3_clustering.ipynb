{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16bc6d3",
   "metadata": {},
   "source": [
    "# Step 3 - K-Modes Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058e6fd",
   "metadata": {},
   "source": [
    "This code requires the following file to run:\n",
    "- St2_person_level_engineered.parquet\n",
    "\n",
    "-------------------------\n",
    "\n",
    "This is the code for the third step of our pipeline.\n",
    "It is divided into two main parts, Clustering and Cluster Visualization. In summary:\n",
    "\n",
    "*For Clustering*\n",
    "* Filter dataset to drivers only\n",
    "* Select relevant categorical and binary features for clustering\n",
    "* Apply K-Modes clustering algorithm for categorical data\n",
    "* Determine optimal number of clusters using elbow method\n",
    "* Evaluate clustering quality using silhouette score and homogeneity metrics\n",
    "* Save cluster assignments for subsequent analysis\n",
    "\n",
    "*For Cluster Visualization*\n",
    "* Visualize cluster characteristics with feature prevalence histograms\n",
    "\n",
    "K-Modes clustering was chosen because it is specifically designed for categorical data, unlike K-Means which only works with numerical data. This algorithm identifies modes (most common values) instead of means, making it ideal for our FARS dataset which contains mostly categorical variables.\n",
    "\n",
    "A limitation of this approach is that the optimal k selection using the elbow method can be subjective (in this code the optimal k does not coincide with the select k for clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8aaf08",
   "metadata": {},
   "source": [
    "### How to run the code:\n",
    "\n",
    "1) Run libraries\n",
    "2) Run all the sections in order (top to bottom)\n",
    "3) Run the Use section\n",
    "4) Optional: review the code of each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cc2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kmodes.kmodes import KModes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e4ef5",
   "metadata": {},
   "source": [
    "### Step 3.1: Data Loading and Filtering\n",
    "\n",
    "Load engineered features and filter to drivers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02298f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_drivers(input_file: Path) -> pd.DataFrame:\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_parquet(input_file)\n",
    "    print(f\"   Total persons in dataset: {len(df):,}\")\n",
    "    \n",
    "    # Filter to drivers only (PER_TYP == 1)\n",
    "    df_drivers = df[df['PER_TYP'] == 1].copy()\n",
    "    print(f\"   Total drivers: {len(df_drivers):,}\")\n",
    "    \n",
    "    return df_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d9d46",
   "metadata": {},
   "source": [
    "### Step 3.2: Feature Selection and Preparation\n",
    "\n",
    "Select relevant features for clustering and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b82033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clustering_features(df: pd.DataFrame, features_config: dict) -> tuple:\n",
    "\n",
    "    # Extract feature configuration (Review USE to modify selected features)\n",
    "    binary_features = features_config.get('binary', [])\n",
    "    categorical_features = features_config.get('categorical', [])\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = binary_features + categorical_features\n",
    "    \n",
    "    print(f\"   Using {len(all_features)} features for clustering:\")\n",
    "    print(f\"      - Binary features: {len(binary_features)}\")\n",
    "    print(f\"      - Categorical features: {len(categorical_features)} ({', '.join(categorical_features)})\")\n",
    "    \n",
    "    # Select features\n",
    "    df_cluster = df[all_features].copy()\n",
    "    \n",
    "    # Handle missing values - drop rows with any missing values\n",
    "    rows_before = len(df_cluster)\n",
    "    df_cluster = df_cluster.dropna()\n",
    "    rows_after = len(df_cluster)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    if rows_removed > 0:\n",
    "        print(f\"\\n   Removed {rows_removed:,} rows with missing values ({rows_removed/rows_before*100:.1f}%)\")\n",
    "    print(f\"   Remaining drivers: {rows_after:,}\")\n",
    "    \n",
    "    return df_cluster, all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b8cf1",
   "metadata": {},
   "source": [
    "### Step 3.3: Elbow Method for Optimal K\n",
    "\n",
    "Determine optimal number of clusters using elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987d18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(X: np.ndarray, k_range: range = range(2, 11)) -> tuple:\n",
    "\n",
    "    # In USE the range of k values can be changed\n",
    "    print(f\"   Testing k values from {min(k_range)} to {max(k_range)}...\")\n",
    "    \n",
    "    costs = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        km = KModes(n_clusters=k, init='Cao', n_init=5, verbose=0, random_state=42)\n",
    "        km.fit(X)\n",
    "        costs[k] = km.cost_\n",
    "        print(f\"      k={k}, cost={km.cost_:.2f}\")\n",
    "    \n",
    "    # Simple elbow detection: find the point with maximum decrease\n",
    "    cost_list = list(costs.values())\n",
    "    k_list = list(costs.keys())\n",
    "    \n",
    "    # Calculate rate of change\n",
    "    rate_changes = []\n",
    "    for i in range(1, len(cost_list)):\n",
    "        rate_changes.append(cost_list[i-1] - cost_list[i])\n",
    "    \n",
    "    # Find elbow (where rate of decrease slows down most)\n",
    "    if len(rate_changes) >= 2:\n",
    "        second_derivatives = []\n",
    "        for i in range(1, len(rate_changes)):\n",
    "            second_derivatives.append(rate_changes[i-1] - rate_changes[i])\n",
    "        optimal_k = k_list[second_derivatives.index(max(second_derivatives)) + 2]\n",
    "    else:\n",
    "        optimal_k = k_list[1]  # Default to k=3 if calculation fails\n",
    "    \n",
    "    print(f\"\\n   Optimal k based on elbow method: {optimal_k}\")\n",
    "    \n",
    "    return optimal_k, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee6c1e",
   "metadata": {},
   "source": [
    "### Step 3.4: K-Modes Clustering\n",
    "\n",
    "*IMPORTANT NOTE:*\n",
    "\n",
    "In this case, the number of clusters was hardcoded (k=6) as it showed similar results and made more sense within the followed context.\n",
    "The code includes the possibility to run the clustering with the automatically detected optimal k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e4236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmodes_clustering(X: np.ndarray, n_clusters: int, verbose: int = 1) -> KModes:\n",
    "\n",
    "    print(f\"\\n   Running final K-Modes clustering with k={n_clusters}...\")\n",
    "    \n",
    "    km = KModes(n_clusters=6, init='Cao', n_init=10, verbose=1, random_state=42) #Hardcoded version\n",
    "    # Automatic version below:\n",
    "    # km = KModes(n_clusters=n_clusters, init='Huang', n_init=1, verbose=verbose)\n",
    "    km.fit(X)\n",
    "    \n",
    "    print(f\"\\n   Clustering complete!\")\n",
    "    print(f\"   Final cost: {km.cost_:.2f}\")\n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe0009",
   "metadata": {},
   "source": [
    "### Step 3.5: Clustering Evaluation\n",
    "\n",
    "Calculate clustering quality metrics (Silhouette Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4546450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X: np.ndarray, labels: np.ndarray, sample_size: int = 10000) -> dict:\n",
    "\n",
    "    # Convert categorical to numeric for silhouette calculation\n",
    "    X_numeric = np.zeros(X.shape)\n",
    "    for i in range(X.shape[1]):\n",
    "        unique_vals = np.unique(X[:, i])\n",
    "        val_to_num = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "        X_numeric[:, i] = [val_to_num[val] for val in X[:, i]]\n",
    "    \n",
    "    # Calculate silhouette score on a sample (for efficiency)\n",
    "    if len(X_numeric) > sample_size:  #Sample size established as 10000\n",
    "        print(f\"   Calculating Silhouette Score (using sample of {sample_size:,} drivers)...\")\n",
    "        sample_idx = np.random.choice(len(X_numeric), sample_size, replace=False)\n",
    "        X_sample = X_numeric[sample_idx]\n",
    "        labels_sample = labels[sample_idx]\n",
    "        silhouette = silhouette_score(X_sample, labels_sample)\n",
    "    else:\n",
    "        print(\"   Calculating Silhouette Score...\")\n",
    "        silhouette = silhouette_score(X_numeric, labels)\n",
    "    \n",
    "    print(f\"      Silhouette Score: {silhouette:.4f}\")\n",
    "    \n",
    "    return {'silhouette_score': silhouette}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b7cf5",
   "metadata": {},
   "source": [
    "### Step 3.6: Results Saving and Summary\n",
    "\n",
    "Save cluster assignments and generate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7198c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustering_results(df_original: pd.DataFrame, df_cluster: pd.DataFrame, \n",
    "                           labels: np.ndarray, output_file: Path, \n",
    "                           costs: dict = None) -> pd.DataFrame:\n",
    "   \n",
    "    # Add cluster labels to the filtered dataframe\n",
    "    df_cluster_result = df_cluster.copy()\n",
    "    df_cluster_result['cluster'] = labels\n",
    "    \n",
    "    # Save results\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_cluster_result.to_parquet(output_path)\n",
    "    print(f\"   Cluster assignments saved to: {output_path}\")\n",
    "    \n",
    "    # Print cluster distribution\n",
    "    print(\"\\n   Cluster size distribution:\")\n",
    "    cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        pct = count / len(labels) * 100\n",
    "        print(f\"      Cluster {cluster_id}: {count:,} drivers ({pct:.1f}%)\")\n",
    "    \n",
    "    # Calculate within-cluster homogeneity\n",
    "    print(\"\\n   Computing within-cluster homogeneity...\")\n",
    "    X_array = df_cluster.values\n",
    "    \n",
    "    for cluster_id in sorted(np.unique(labels)):\n",
    "        cluster_mask = labels == cluster_id\n",
    "        cluster_data = X_array[cluster_mask]\n",
    "        \n",
    "        # Calculate mode agreement for each feature\n",
    "        agreements = []\n",
    "        for col_idx in range(cluster_data.shape[1]):\n",
    "            col_data = cluster_data[:, col_idx]\n",
    "            mode_val = pd.Series(col_data).mode()[0]\n",
    "            agreement = (col_data == mode_val).sum() / len(col_data)\n",
    "            agreements.append(agreement)\n",
    "        \n",
    "        avg_homogeneity = np.mean(agreements) * 100\n",
    "        print(f\"      Cluster {cluster_id}: {avg_homogeneity:.1f}% average feature agreement\")\n",
    "    \n",
    "    overall_homogeneity = np.mean([np.mean([(X_array[labels == c, col] == \n",
    "                                             pd.Series(X_array[labels == c, col]).mode()[0]).sum() / \n",
    "                                            (labels == c).sum() \n",
    "                                            for col in range(X_array.shape[1])]) \n",
    "                                  for c in np.unique(labels)]) * 100\n",
    "    print(f\"\\n   Overall average homogeneity: {overall_homogeneity:.1f}%\")\n",
    "    \n",
    "    # Save elbow plot if costs provided\n",
    "    if costs is not None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        k_values = list(costs.keys())\n",
    "        cost_values = list(costs.values())\n",
    "        \n",
    "        plt.plot(k_values, cost_values, 'bo-', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Cost', fontsize=12, fontweight='bold')\n",
    "        plt.title('K-Modes Elbow Method', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_path = Path(output_file)\n",
    "\n",
    "        elbow_plot_path = Path(\"Results\") / \"Optimal_K\" / \"kmodes_elbow.png\"\n",
    "        plt.savefig(elbow_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\n   Elbow plot saved to: {elbow_plot_path}\")\n",
    "    \n",
    "    return df_cluster_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c11d97",
   "metadata": {},
   "source": [
    "### Step 3.7: Cluster Visualization\n",
    "\n",
    "Create visualizations for cluster characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd22fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_features(df: pd.DataFrame, cluster_id: int, \n",
    "                          binary_cols: list, categorical_cols: list) -> dict:\n",
    "\n",
    "    cluster_data = df[df['cluster'] == cluster_id]\n",
    "    features = {}\n",
    "    \n",
    "    # Binary features (show both 0 and 1)\n",
    "    for col in binary_cols:\n",
    "        if col in cluster_data.columns:\n",
    "            features[f\"{col}=1\"] = (cluster_data[col] == 1).sum() / len(cluster_data) * 100\n",
    "            features[f\"{col}=0\"] = (cluster_data[col] == 0).sum() / len(cluster_data) * 100\n",
    "    \n",
    "    # Categorical features (show all categories)\n",
    "    for col in categorical_cols:\n",
    "        if col in cluster_data.columns:\n",
    "            value_counts = cluster_data[col].value_counts(normalize=True, dropna=False) * 100\n",
    "            for val, pct in value_counts.items():\n",
    "                features[f\"{col}={val}\"] = pct\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def visualize_clusters(df_clusters: pd.DataFrame, binary_features: list, \n",
    "                      categorical_features: list, output_dir: Path, \n",
    "                      prevalence_threshold: float = 65.0):\n",
    " \n",
    "    clusters = sorted(df_clusters['cluster'].unique())\n",
    "    n_clusters = len(clusters)\n",
    "    print(f\"   Number of clusters: {n_clusters}\")\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True)\n",
    "    \n",
    "    for cluster_id in clusters:\n",
    "        # Get feature prevalence for this cluster\n",
    "        features_dict = create_binary_features(df_clusters, cluster_id, \n",
    "                                               binary_features, categorical_features)\n",
    "        \n",
    "        # Filter to high prevalence features\n",
    "        high_prev_features = {k: v for k, v in features_dict.items() \n",
    "                             if v > prevalence_threshold}\n",
    "        \n",
    "        # Sort by prevalence (descending)\n",
    "        sorted_features = dict(sorted(high_prev_features.items(), \n",
    "                                     key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        # Get cluster size\n",
    "        cluster_size = len(df_clusters[df_clusters['cluster'] == cluster_id])\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(14, max(6, len(sorted_features) * 0.4)))\n",
    "        \n",
    "        if len(sorted_features) > 0:\n",
    "            names = list(sorted_features.keys())\n",
    "            values = list(sorted_features.values())\n",
    "            \n",
    "            # Create horizontal bar chart\n",
    "            bars = ax.barh(names, values, color='steelblue', \n",
    "                          edgecolor='navy', linewidth=0.5)\n",
    "            \n",
    "            # Styling\n",
    "            ax.set_xlabel('Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(\n",
    "                f'Cluster {cluster_id}: Features >{prevalence_threshold}% Prevalent '\n",
    "                f'(n={cluster_size:,} drivers)',\n",
    "                fontsize=14, fontweight='bold', pad=15\n",
    "            )\n",
    "            ax.axvline(x=prevalence_threshold, color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'{prevalence_threshold}% threshold')\n",
    "            ax.set_xlim(prevalence_threshold-5, 105)\n",
    "            ax.legend(loc='lower right', fontsize=10)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, val in zip(bars, values):\n",
    "                ax.text(val + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{val:.1f}%', va='center', ha='left', \n",
    "                       fontsize=9, fontweight='bold')\n",
    "            \n",
    "            ax.tick_params(axis='y', labelsize=10)\n",
    "            ax.tick_params(axis='x', labelsize=10)\n",
    "        else:\n",
    "            # No high-prevalence features\n",
    "            ax.text(0.5, 0.5, \n",
    "                   f'No features >{prevalence_threshold}% prevalent\\n'\n",
    "                   f'(Cluster size: {cluster_size:,} drivers)',\n",
    "                   ha='center', va='center', fontsize=13, fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        output_file = Path(\"Results\") / \"Individual_Cluster_Results\" / f\"cluster_{cluster_id}_features.png\"\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   Cluster {cluster_id}: {len(sorted_features)} features \"\n",
    "              f\">{prevalence_threshold}% prevalent\")\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"\\n   Visualizations saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc1223",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Run the complete clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97968ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering_pipeline(\n",
    "    input_file: Path,\n",
    "    output_file: Path,\n",
    "    features_config: dict,\n",
    "    k_range: range = range(2, 11),\n",
    "    visualize: bool = True,\n",
    "    prevalence_threshold: float = 65.0\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Step 1: Load and filter to drivers\n",
    "    df_drivers = load_and_filter_drivers(input_file)\n",
    "    \n",
    "    # Step 2: Prepare features\n",
    "    df_cluster, feature_names = prepare_clustering_features(df_drivers, features_config)\n",
    "    \n",
    "    # Step 3: Prepare data summary\n",
    "\n",
    "    # Get accident-level statistics\n",
    "    if 'ST_CASE' in df_drivers.columns:\n",
    "        unique_accidents = df_cluster.index.map(\n",
    "            lambda idx: df_drivers.loc[idx, 'ST_CASE'] \n",
    "            if idx in df_drivers.index else None\n",
    "        ).nunique()\n",
    "        print(f\"   Total drivers for clustering: {len(df_cluster):,}\")\n",
    "        print(f\"   Unique accidents represented: {unique_accidents:,}\")\n",
    "        print(f\"   Average drivers per accident: {len(df_cluster)/unique_accidents:.2f}\")\n",
    "    print(f\"   Final clustering dataset shape: {df_cluster.shape}\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    X = df_cluster.values\n",
    "    \n",
    "    # Step 4: Find optimal k\n",
    "    optimal_k, costs = find_optimal_k(X, k_range)\n",
    "    \n",
    "    # Step 5: Perform clustering\n",
    "    km_model = perform_kmodes_clustering(X, optimal_k, verbose=1)\n",
    "    \n",
    "    # Step 6: Evaluate\n",
    "    metrics = evaluate_clustering(X, km_model.labels_)\n",
    "    \n",
    "    # Step 7: Save results\n",
    "    df_results = save_clustering_results(\n",
    "        df_drivers, df_cluster, km_model.labels_, output_file, costs\n",
    "    )\n",
    "    \n",
    "    # Step 8: Visualize (optional)\n",
    "    if visualize:\n",
    "        visualize_clusters(\n",
    "            df_results, \n",
    "            features_config['binary'],\n",
    "            features_config['categorical'],\n",
    "            Path(output_file),\n",
    "            prevalence_threshold\n",
    "        )\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"   • Total drivers clustered: {len(df_cluster):,}\")\n",
    "    print(f\"   • Number of clusters: {optimal_k}\")\n",
    "    print(f\"   • Silhouette score: {metrics['silhouette_score']:.4f}\")\n",
    "    print(f\"   • Features used: {len(feature_names)}\")\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"   • {output_file}\")\n",
    "    if visualize:\n",
    "        print(f\"   • Results\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542de98",
   "metadata": {},
   "source": [
    "### USE\n",
    "\n",
    "Configure INPUT and OUTPUT paths and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7a47e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total persons in dataset: 92,400\n",
      "   Total drivers: 57,939\n",
      "   Using 18 features for clustering:\n",
      "      - Binary features: 15\n",
      "      - Categorical features: 3 (TIME_OF_DAY, SEASON, AGE_GROUP)\n",
      "\n",
      "   Removed 2,214 rows with missing values (3.8%)\n",
      "   Remaining drivers: 55,725\n",
      "   Total drivers for clustering: 55,725\n",
      "   Unique accidents represented: 35,995\n",
      "   Average drivers per accident: 1.55\n",
      "   Final clustering dataset shape: (55725, 18)\n",
      "   Testing k values from 2 to 10...\n",
      "      k=2, cost=269056.00\n",
      "      k=3, cost=256796.00\n",
      "      k=4, cost=242167.00\n",
      "      k=5, cost=234373.00\n",
      "      k=6, cost=227144.00\n",
      "      k=7, cost=227008.00\n",
      "      k=8, cost=221670.00\n",
      "      k=9, cost=214055.00\n",
      "      k=10, cost=207694.00\n",
      "\n",
      "   Optimal k based on elbow method: 7\n",
      "\n",
      "   Running final K-Modes clustering with k=7...\n",
      "Initialization method and algorithm are deterministic. Setting n_init to 1.\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 14829, cost: 229227.0\n",
      "Run 1, iteration: 2/100, moves: 5974, cost: 227144.0\n",
      "Run 1, iteration: 3/100, moves: 416, cost: 227144.0\n",
      "\n",
      "   Clustering complete!\n",
      "   Final cost: 227144.00\n",
      "   Calculating Silhouette Score (using sample of 10,000 drivers)...\n",
      "      Silhouette Score: 0.0143\n",
      "   Cluster assignments saved to: Dataset\\St3_fatal_accident_clusters.parquet\n",
      "\n",
      "   Cluster size distribution:\n",
      "      Cluster 0: 26,348 drivers (47.3%)\n",
      "      Cluster 1: 5,653 drivers (10.1%)\n",
      "      Cluster 2: 9,296 drivers (16.7%)\n",
      "      Cluster 3: 4,804 drivers (8.6%)\n",
      "      Cluster 4: 6,768 drivers (12.1%)\n",
      "      Cluster 5: 2,856 drivers (5.1%)\n",
      "\n",
      "   Computing within-cluster homogeneity...\n",
      "      Cluster 0: 76.6% average feature agreement\n",
      "      Cluster 1: 74.7% average feature agreement\n",
      "      Cluster 2: 77.7% average feature agreement\n",
      "      Cluster 3: 80.0% average feature agreement\n",
      "      Cluster 4: 80.4% average feature agreement\n",
      "      Cluster 5: 77.3% average feature agreement\n",
      "\n",
      "   Overall average homogeneity: 77.8%\n",
      "\n",
      "   Elbow plot saved to: Results\\Optimal_K\\kmodes_elbow.png\n",
      "   Number of clusters: 6\n",
      "   Cluster 0: 14 features >65.0% prevalent\n",
      "   Cluster 1: 12 features >65.0% prevalent\n",
      "   Cluster 2: 13 features >65.0% prevalent\n",
      "   Cluster 3: 14 features >65.0% prevalent\n",
      "   Cluster 4: 14 features >65.0% prevalent\n",
      "   Cluster 5: 12 features >65.0% prevalent\n",
      "\n",
      "   Visualizations saved to: Dataset\\St3_fatal_accident_clusters.parquet\n",
      "\n",
      "Summary:\n",
      "   • Total drivers clustered: 55,725\n",
      "   • Number of clusters: 7\n",
      "   • Silhouette score: 0.0143\n",
      "   • Features used: 18\n",
      "\n",
      "Output files:\n",
      "   • Dataset\\St3_fatal_accident_clusters.parquet\n",
      "   • Results\n"
     ]
    }
   ],
   "source": [
    "# INPUT/OUTPUT Configuration\n",
    "INPUT_FILE = Path(\"Dataset/St2_person_level_engineered.parquet\")\n",
    "OUTPUT_FILE = Path(\"Dataset/St3_fatal_accident_clusters.parquet\")\n",
    "\n",
    "# Features Configuration (the ones used)\n",
    "FEATURES_CONFIG = {\n",
    "    'binary': [\n",
    "        'RUSH_HOUR',\n",
    "        'MALE',\n",
    "        'ADVERSE_WEATHER',\n",
    "        'DARK_CONDITIONS',\n",
    "        'OLD_VEHICLE',\n",
    "        'PASSENGER_CAR',\n",
    "        'LARGE_TRUCK',\n",
    "        'MOTORCYCLE',\n",
    "        'URBAN',\n",
    "        'INTERSTATE',\n",
    "        'INTERSECTION',\n",
    "        'WORK_ZONE_CRASH',\n",
    "        'ROLLOVER_CRASH',\n",
    "        'FIRE',\n",
    "        'WEEKEND_FLAG'\n",
    "    ],\n",
    "    'categorical': [\n",
    "        'TIME_OF_DAY',\n",
    "        'SEASON',\n",
    "        'AGE_GROUP'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Clustering Parameters\n",
    "K_RANGE = range(2, 11)  # Test k from 2 to 10\n",
    "VISUALIZE = True  # Create cluster visualizations\n",
    "PREVALENCE_THRESHOLD = 65.0  # Show features above this % in visualizations\n",
    "\n",
    "# RUN PIPELINE\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    print(f\"\\nInput file not found: {INPUT_FILE}\")\n",
    "    print(\"Run Step 2 (Feature Engineering) first\")\n",
    "else:\n",
    "    df_clustered = run_clustering_pipeline(\n",
    "        input_file=INPUT_FILE,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        features_config=FEATURES_CONFIG,\n",
    "        k_range=K_RANGE,\n",
    "        visualize=VISUALIZE,\n",
    "        prevalence_threshold=PREVALENCE_THRESHOLD\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
